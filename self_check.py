#!/usr/bin/env python3
"""
è‡ªæµ‹è„šæœ¬ - è¯„ä¼°AIåˆ†æå›ç­”è´¨é‡
ç”¨äºä»£ç è°ƒæ•´åè‡ªåŠ¨éªŒè¯åˆ†æè´¨é‡
"""
import asyncio
import json
import logging
import re
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass

from agent.simple_planner import get_planner
from agent.functions import get_executor
from agent.analyzer import analyze_result

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class TestCase:
    """æµ‹è¯•ç”¨ä¾‹"""
    id: str
    question: str
    expected_keywords: List[str]  # é¢„æœŸåŒ…å«çš„å…³é”®è¯
    expected_focus: str  # é¢„æœŸåˆ†æé‡ç‚¹
    description: str  # æµ‹è¯•ç”¨ä¾‹æè¿°
    min_length: int = 100  # æœ€å°å›ç­”é•¿åº¦


@dataclass
class TestResult:
    """æµ‹è¯•ç»“æœ"""
    test_id: str
    question: str
    answer: str
    sql: str
    passed: bool
    score: float  # 0-100åˆ†
    issues: List[str]
    execution_time: float


class AnswerQualityChecker:
    """å›ç­”è´¨é‡æ£€æŸ¥å™¨"""
    
    def __init__(self):
        self.passed_tests = 0
        self.failed_tests = 0
        self.results: List[TestResult] = []
    
    def check_relevance(self, answer: str, question: str, expected_focus: str) -> Tuple[bool, List[str]]:
        """æ£€æŸ¥å›ç­”ç›¸å…³æ€§"""
        issues = []
        
        # æ£€æŸ¥æ˜¯å¦é’ˆå¯¹ç”¨æˆ·é—®é¢˜
        if "è®¾å¤‡æ•°é‡" in question and "device_count" not in answer.lower():
            if "count" in answer.lower() and "device" not in answer.lower():
                issues.append("å¯èƒ½åˆ†æäº†é”™è¯¯çš„æ•°æ®åˆ—ï¼ˆcountè€Œédevice_countï¼‰")
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«é¢„æœŸçš„åˆ†æé‡ç‚¹
        if expected_focus.lower() not in answer.lower():
            issues.append(f"ç¼ºå°‘é¢„æœŸçš„åˆ†æé‡ç‚¹ï¼š{expected_focus}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç©ºè¯å¥—è¯
        empty_phrases = ["æ€»çš„æ¥è¯´", "ç»¼ä¸Šæ‰€è¿°", "å€¼å¾—æ³¨æ„çš„æ˜¯", "éœ€è¦æŒ‡å‡ºçš„æ˜¯"]
        if sum(1 for phrase in empty_phrases if phrase in answer) > 2:
            issues.append("åŒ…å«è¿‡å¤šç©ºè¯å¥—è¯")
        
        return len(issues) == 0, issues
    
    def check_data_accuracy(self, answer: str) -> Tuple[bool, List[str]]:
        """æ£€æŸ¥æ•°æ®å‡†ç¡®æ€§"""
        issues = []
        
        # æŸ¥æ‰¾ç™¾åˆ†æ¯”æ•°å­—ï¼Œæ£€æŸ¥æ˜¯å¦åˆç†
        percentage_pattern = r'(\d+(?:\.\d+)?)\s*%|ç™¾åˆ†æ¯”'
        percentages = re.findall(percentage_pattern, answer)
        
        for pct in percentages:
            try:
                pct_float = float(pct)
                if pct_float > 100:
                    issues.append(f"å¼‚å¸¸çš„ç™¾åˆ†æ¯”å€¼ï¼š{pct}%")
                elif pct_float < 0:
                    issues.append(f"è´Ÿæ•°çš„ç™¾åˆ†æ¯”å€¼ï¼š{pct}%")
            except ValueError:
                continue
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç¼–é€ æ•°æ®çš„å«Œç–‘
        if "çº¦" in answer and "%" in answer:
            # ç®€å•æ£€æŸ¥ï¼šé¿å…è¿‡å¤šæ¨¡ç³Šè¡¨è¿°
            vague_count = answer.count("çº¦") + answer.count("å¤§æ¦‚") + answer.count("å·¦å³")
            if vague_count > 3:
                issues.append("æ¨¡ç³Šè¡¨è¿°è¿‡å¤šï¼Œå¯èƒ½å½±å“æ•°æ®å‡†ç¡®æ€§")
        
        return len(issues) == 0, issues
    
    def check_value_orientation(self, answer: str, expected_keywords: List[str]) -> Tuple[bool, List[str]]:
        """æ£€æŸ¥ä»·å€¼å¯¼å‘"""
        issues = []
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®è¯
        found_keywords = sum(1 for keyword in expected_keywords if keyword in answer)
        if found_keywords < len(expected_keywords) / 2:
            issues.append(f"ç¼ºå°‘é¢„æœŸçš„å…³é”®è¯ï¼š{expected_keywords}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å…·ä½“å»ºè®®
        if "å»ºè®®" not in answer and "ä¼˜åŒ–" not in answer:
            if "é—®é¢˜" in answer:  # å¦‚æœæåˆ°äº†é—®é¢˜ä½†æ²¡æœ‰å»ºè®®
                issues.append("æŒ‡å‡ºäº†é—®é¢˜ä½†æœªæä¾›å…·ä½“å»ºè®®")
        
        # æ£€æŸ¥åˆ†æç»“æ„
        required_structures = ["å‘ç°", "æ•°æ®", "åˆ†æ"]
        structure_score = sum(1 for structure in required_structures if structure in answer)
        if structure_score < 2:
            issues.append("åˆ†æç»“æ„ä¸å¤Ÿå®Œæ•´")
        
        return len(issues) == 0, issues
    
    def check_length_quality(self, answer: str, min_length: int) -> Tuple[bool, List[str]]:
        """æ£€æŸ¥é•¿åº¦è´¨é‡"""
        issues = []
        
        if len(answer) < min_length:
            issues.append(f"å›ç­”è¿‡çŸ­ï¼ˆ{len(answer)}å­—ç¬¦ï¼‰ï¼Œè‡³å°‘éœ€è¦{min_length}å­—ç¬¦")
        elif len(answer) > 800:
            issues.append("å›ç­”è¿‡é•¿ï¼Œå¯èƒ½åŒ…å«å†—ä½™ä¿¡æ¯")
        
        # æ£€æŸ¥å¥å­å®Œæ•´æ€§
        sentences = answer.split("ã€‚")
        incomplete_sentences = sum(1 for s in sentences if len(s.strip()) < 5)
        if incomplete_sentences > len(sentences) * 0.3:
            issues.append("å­˜åœ¨è¿‡å¤šä¸å®Œæ•´çš„å¥å­")
        
        return len(issues) == 0, issues
    
    def evaluate_answer(self, test_case: TestCase, answer: str, sql: str) -> TestResult:
        """ç»¼åˆè¯„ä¼°å›ç­”è´¨é‡"""
        start_time = time.time()
        
        all_issues = []
        
        # ç›¸å…³æ€§æ£€æŸ¥
        relevance_ok, relevance_issues = self.check_relevance(answer, test_case.question, test_case.expected_focus)
        all_issues.extend(relevance_issues)
        
        # æ•°æ®å‡†ç¡®æ€§æ£€æŸ¥
        accuracy_ok, accuracy_issues = self.check_data_accuracy(answer)
        all_issues.extend(accuracy_issues)
        
        # ä»·å€¼å¯¼å‘æ£€æŸ¥
        value_ok, value_issues = self.check_value_orientation(answer, test_case.expected_keywords)
        all_issues.extend(value_issues)
        
        # é•¿åº¦è´¨é‡æ£€æŸ¥
        length_ok, length_issues = self.check_length_quality(answer, test_case.min_length)
        all_issues.extend(length_issues)
        
        # è®¡ç®—æ€»åˆ†
        passed_checks = sum([relevance_ok, accuracy_ok, value_ok, length_ok])
        total_checks = 4
        base_score = (passed_checks / total_checks) * 100
        
        # æ ¹æ®é—®é¢˜æ•°é‡è°ƒæ•´åˆ†æ•°
        if len(all_issues) == 0:
            score = 100
        else:
            score = max(0, base_score - (len(all_issues) * 5))
        
        execution_time = time.time() - start_time
        
        result = TestResult(
            test_id=test_case.id,
            question=test_case.question,
            answer=answer,
            sql=sql,
            passed=score >= 70,  # 70åˆ†ä»¥ä¸Šç®—é€šè¿‡
            score=score,
            issues=all_issues,
            execution_time=execution_time
        )
        
        if result.passed:
            self.passed_tests += 1
        else:
            self.failed_tests += 1
        
        self.results.append(result)
        return result


class SelfCheckRunner:
    """è‡ªæµ‹è¿è¡Œå™¨"""
    
    def __init__(self):
        self.quality_checker = AnswerQualityChecker()
    
    def load_test_cases(self, test_file: str = "test_cases.json") -> List[TestCase]:
        """åŠ è½½æµ‹è¯•ç”¨ä¾‹"""
        test_path = Path(test_file)
        
        if test_path.exists():
            with open(test_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return [TestCase(**case) for case in data["test_cases"]]
        
        # é»˜è®¤æµ‹è¯•ç”¨ä¾‹
        return self._get_default_test_cases()
    
    def _get_default_test_cases(self) -> List[TestCase]:
        """è·å–é»˜è®¤æµ‹è¯•ç”¨ä¾‹"""
        return [
            TestCase(
                id="test_001",
                question="æŸ¥è¯¢è¿‘3hï¼Œå„ä¸ªè¿è¥å•†çš„æ¢æµ‹è®¾å¤‡æ•°é‡ï¼Œç”¨tableçš„æ–¹å¼è¾“å‡º",
                expected_keywords=["è®¾å¤‡", "æ•°é‡", "è¿è¥å•†", "åˆ†å¸ƒ"],
                expected_focus="è®¾å¤‡æ•°é‡åˆ†å¸ƒ",
                description="æµ‹è¯•è®¾å¤‡æ•°é‡åˆ†æçš„å‡†ç¡®æ€§"
            ),
            TestCase(
                id="test_002", 
                question="åˆ†ææµ™æ±Ÿç”µä¿¡è¿‘1hçš„ç½‘ç»œè´¨é‡æƒ…å†µ",
                expected_keywords=["å»¶è¿Ÿ", "ä¸¢åŒ…", "ç½‘ç»œè´¨é‡", "æ€§èƒ½"],
                expected_focus="ç½‘ç»œè´¨é‡åˆ†æ",
                description="æµ‹è¯•ç½‘ç»œè´¨é‡åˆ†æçš„ä¸“ä¸šæ€§"
            ),
            TestCase(
                id="test_003",
                question="å¯¹æ¯”å„ä¸ªçœä»½æ¢æµ‹ç»“æœçš„è¦†ç›–ç‡",
                expected_keywords=["è¦†ç›–", "çœä»½", "åˆ†å¸ƒ", "å¯¹æ¯”"],
                expected_focus="è¦†ç›–ç‡åˆ†æ",
                description="æµ‹è¯•åœ°åŒºè¦†ç›–åˆ†æçš„å…¨é¢æ€§"
            ),
            TestCase(
                id="test_004",
                question="æŸ¥çœ‹æœ€è¿‘æ¢æµ‹ä»»åŠ¡ä¸­å»¶è¿Ÿæœ€é«˜çš„é—®é¢˜èŠ‚ç‚¹",
                expected_keywords=["å»¶è¿Ÿ", "èŠ‚ç‚¹", "é—®é¢˜", "æ€§èƒ½"],
                expected_focus="é—®é¢˜èŠ‚ç‚¹è¯†åˆ«",
                description="æµ‹è¯•é—®é¢˜å‘ç°å’Œå®šä½èƒ½åŠ›"
            ),
            TestCase(
                id="test_005",
                question="åˆ†æä¸åŒæ—¶æ®µçš„ç½‘ç»œæ€§èƒ½å˜åŒ–è¶‹åŠ¿",
                expected_keywords=["æ—¶æ®µ", "è¶‹åŠ¿", "æ€§èƒ½", "å˜åŒ–"],
                expected_focus="è¶‹åŠ¿åˆ†æ",
                description="æ—¶é—´åºåˆ—åˆ†æèƒ½åŠ›"
            )
        ]
    
    async def run_single_test(self, test_case: TestCase) -> TestResult:
        """è¿è¡Œå•ä¸ªæµ‹è¯•"""
        logger.info(f"æ‰§è¡Œæµ‹è¯• {test_case.id}: {test_case.description}")
        logger.info(f"é—®é¢˜: {test_case.question}")
        
        try:
            # 1. ç”ŸæˆæŸ¥è¯¢è®¡åˆ’
            planner = get_planner()
            query_plan = planner.plan(test_case.question)
            
            # 2. æ‰§è¡ŒæŸ¥è¯¢
            executor = get_executor()
            df = executor.run_query(query_plan)
            sql = executor.get_generated_sql(query_plan)
            
            # 3. ç”Ÿæˆåˆ†æ
            answer = executor.explain_result(df, query_plan)
            
            # 4. è¯„ä¼°è´¨é‡
            result = self.quality_checker.evaluate_answer(test_case, answer, sql)
            
            logger.info(f"æµ‹è¯• {test_case.id} å®Œæˆï¼Œå¾—åˆ†: {result.score:.1f}")
            if not result.passed:
                logger.warning(f"é—®é¢˜: {'; '.join(result.issues)}")
            
            return result
            
        except Exception as e:
            logger.error(f"æµ‹è¯• {test_case.id} æ‰§è¡Œå¤±è´¥: {e}")
            return TestResult(
                test_id=test_case.id,
                question=test_case.question,
                answer=f"æ‰§è¡Œå¤±è´¥: {str(e)}",
                sql="",
                passed=False,
                score=0,
                issues=[f"æ‰§è¡Œå¼‚å¸¸: {str(e)}"],
                execution_time=0
            )
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹è‡ªæµ‹...")
        
        test_cases = self.load_test_cases()
        start_time = time.time()
        
        # å¹¶è¡Œæ‰§è¡Œæµ‹è¯•
        tasks = [self.run_single_test(case) for case in test_cases]
        results = await asyncio.gather(*tasks)
        
        total_time = time.time() - start_time
        
        # ç»Ÿè®¡ç»“æœ
        passed_count = self.quality_checker.passed_tests
        failed_count = self.quality_checker.failed_tests
        total_count = len(results)
        pass_rate = (passed_count / total_count * 100) if total_count > 0 else 0
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total": total_count,
                "passed": passed_count,
                "failed": failed_count,
                "pass_rate": pass_rate,
                "execution_time": total_time
            },
            "details": []
        }
        
        for result in results:
            report["details"].append({
                "test_id": result.test_id,
                "question": result.question,
                "passed": result.passed,
                "score": result.score,
                "issues": result.issues,
                "execution_time": result.execution_time,
                "answer_length": len(result.answer)
            })
        
        return report
    
    def save_report(self, report: Dict[str, Any], output_file: str = "self_check_report.json"):
        """ä¿å­˜æµ‹è¯•æŠ¥å‘Š"""
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ“Š æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
    
    def print_summary(self, report: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        summary = report["summary"]
        
        print("\n" + "="*60)
        print("ğŸ§ª è‡ªæµ‹å®Œæˆï¼")
        print("="*60)
        print(f"æµ‹è¯•æ€»æ•°: {summary['total']}")
        print(f"é€šè¿‡æ•°é‡: {summary['passed']}")
        print(f"å¤±è´¥æ•°é‡: {summary['failed']}")
        print(f"é€šè¿‡ç‡: {summary['pass_rate']:.1f}%")
        print(f"æ‰§è¡Œæ—¶é—´: {summary['execution_time']:.2f}ç§’")
        
        if summary['failed'] > 0:
            print(f"\nâŒ å¤±è´¥çš„æµ‹è¯•:")
            for detail in report["details"]:
                if not detail["passed"]:
                    print(f"  - {detail['test_id']}: {detail['question'][:30]}...")
                    if detail["issues"]:
                        print(f"    é—®é¢˜: {'; '.join(detail['issues'])}")
        
        print("\n" + "="*60)


async def main():
    """ä¸»å‡½æ•°"""
    runner = SelfCheckRunner()
    
    try:
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        report = await runner.run_all_tests()
        
        # æ‰“å°æ‘˜è¦
        runner.print_summary(report)
        
        # ä¿å­˜æŠ¥å‘Š
        runner.save_report(report)
        
        # å¦‚æœé€šè¿‡ç‡ä½äº70%ï¼Œè¿”å›é0é€€å‡ºç 
        if report["summary"]["pass_rate"] < 70:
            logger.warning("âš ï¸  è‡ªæµ‹æœªé€šè¿‡ï¼Œåˆ†æè´¨é‡éœ€è¦æ”¹è¿›")
            exit(1)
        else:
            logger.info("âœ… è‡ªæµ‹é€šè¿‡ï¼Œåˆ†æè´¨é‡ç¬¦åˆè¦æ±‚")
            
    except Exception as e:
        logger.error(f"è‡ªæµ‹æ‰§è¡Œå¤±è´¥: {e}")
        exit(1)


if __name__ == "__main__":
    asyncio.run(main())